{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ded031",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337704bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcaba0b",
   "metadata": {},
   "source": [
    "#### Setup NLTK and Text Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe72737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def load_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "    return text\n",
    "\n",
    "text_data = load_text(\"outs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc59e02",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b8721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the words corpus\n",
    "nltk.download('words')\n",
    "\n",
    "# Load the English words corpus\n",
    "english_words = set(words.words())\n",
    "\n",
    "def clean_text_and_split(text):\n",
    "    # Remove newline characters\n",
    "    cleaned_text = text.replace('\\n', ' ')\n",
    "    # Remove extra spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    # Add period to the end of sentences\n",
    "    cleaned_text = re.sub(r'(?<=[a-zA-Z0-9])\\n(?=[A-Z])', '. ', cleaned_text)\n",
    "    # Add period to the end of text\n",
    "    cleaned_text = re.sub(r'(?<=[a-zA-Z0-9])\\n*$', '.', cleaned_text)\n",
    "    # Remove punctuation and symbols\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', ' ', cleaned_text)\n",
    "    # Remove multiple spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    # Split the cleaned text into words\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    # Spell check and filter out non-English words\n",
    "    corrected_tokens = [word for word in tokens if word.lower() in english_words]\n",
    "    \n",
    "    return corrected_tokens\n",
    "\n",
    "token = clean_text_and_split(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11805dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'a',\n",
       " 'b',\n",
       " 'high',\n",
       " 'performance',\n",
       " 'was',\n",
       " 'a',\n",
       " 'competition',\n",
       " 'pistol',\n",
       " 'manufacturer',\n",
       " 'included',\n",
       " 'the',\n",
       " 'limited',\n",
       " 'class',\n",
       " 'and',\n",
       " 'open',\n",
       " 'class',\n",
       " 'semi',\n",
       " 'automatic',\n",
       " 'both',\n",
       " 'available',\n",
       " 'in',\n",
       " 's',\n",
       " 'w',\n",
       " 'and',\n",
       " 'a',\n",
       " 'b',\n",
       " 'sold',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'external',\n",
       " 'links',\n",
       " 'category',\n",
       " 'defunct',\n",
       " 'category',\n",
       " 'defunct',\n",
       " 'based',\n",
       " 'in',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'is',\n",
       " 'a',\n",
       " 'book',\n",
       " 'company',\n",
       " 'since',\n",
       " 'by',\n",
       " 'the',\n",
       " 'company',\n",
       " 'is',\n",
       " 'noted',\n",
       " 'for',\n",
       " 'who',\n",
       " 's',\n",
       " 'who',\n",
       " 'since',\n",
       " 'and',\n",
       " 'the',\n",
       " 'encyclopedia',\n",
       " 'between',\n",
       " 'and',\n",
       " 'it',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'in',\n",
       " 'fiction',\n",
       " 'and',\n",
       " 'nonfiction',\n",
       " 'and',\n",
       " 'popular',\n",
       " 'travel',\n",
       " 'and',\n",
       " 'science',\n",
       " 'history',\n",
       " 'the',\n",
       " 'firm',\n",
       " 'was',\n",
       " 'in',\n",
       " 'by',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'in',\n",
       " 'the',\n",
       " 'company',\n",
       " 'the',\n",
       " 'to',\n",
       " 'sir',\n",
       " 'walter',\n",
       " 's',\n",
       " 'for',\n",
       " 'the',\n",
       " 'company',\n",
       " 'to',\n",
       " 'the',\n",
       " 'soho',\n",
       " 'district',\n",
       " 'of',\n",
       " 'in',\n",
       " 'during',\n",
       " 'the',\n",
       " 'the',\n",
       " 'firm',\n",
       " 'the',\n",
       " 'seventh',\n",
       " 'eighth',\n",
       " 'and',\n",
       " 'ninth',\n",
       " 'of',\n",
       " 'the',\n",
       " 'this',\n",
       " 'was',\n",
       " 'from',\n",
       " 'constable',\n",
       " 'after',\n",
       " 'his',\n",
       " 'company',\n",
       " 's',\n",
       " 'failure',\n",
       " 'to',\n",
       " 'publish',\n",
       " 'the',\n",
       " 'seventh',\n",
       " 'edition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'encyclopedia',\n",
       " 'black',\n",
       " 'retired',\n",
       " 'in',\n",
       " 'due',\n",
       " 'to',\n",
       " 'his',\n",
       " 'disapproval',\n",
       " 'of',\n",
       " 'his',\n",
       " 'sons',\n",
       " 'extravagant',\n",
       " 'for',\n",
       " 'its',\n",
       " 'ninth',\n",
       " 'edition',\n",
       " 'this',\n",
       " 'edition',\n",
       " 'however',\n",
       " 'would',\n",
       " 'sell',\n",
       " 'half',\n",
       " 'a',\n",
       " 'million',\n",
       " 'and',\n",
       " 'was',\n",
       " 'in',\n",
       " 'from',\n",
       " 'to',\n",
       " 'beginning',\n",
       " 'in',\n",
       " 'the',\n",
       " 'firm',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'travel',\n",
       " 'known',\n",
       " 'as',\n",
       " 'black',\n",
       " 's',\n",
       " 'the',\n",
       " 'company',\n",
       " 'was',\n",
       " 'the',\n",
       " 'publisher',\n",
       " 'of',\n",
       " 'the',\n",
       " 'annual',\n",
       " 'who',\n",
       " 's',\n",
       " 'who',\n",
       " 'since',\n",
       " 'and',\n",
       " 'also',\n",
       " 'since',\n",
       " 'the',\n",
       " 's',\n",
       " 'other',\n",
       " 'notable',\n",
       " 'works',\n",
       " 'include',\n",
       " 'black',\n",
       " 's',\n",
       " 'medical',\n",
       " 'dictionary',\n",
       " 'and',\n",
       " 'the',\n",
       " 'know',\n",
       " 'the',\n",
       " 'game',\n",
       " 'series',\n",
       " 'of',\n",
       " 'sports',\n",
       " 'and',\n",
       " 'reference',\n",
       " 'know',\n",
       " 'the',\n",
       " 'game',\n",
       " 'archive',\n",
       " 'the',\n",
       " 'firm',\n",
       " 'also',\n",
       " 'the',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'colour',\n",
       " 'twenty',\n",
       " 'shilling',\n",
       " 'series',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'colour',\n",
       " 'twenty',\n",
       " 'shilling',\n",
       " 'series',\n",
       " 'on',\n",
       " 'a',\n",
       " 'range',\n",
       " 'of',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'colour',\n",
       " 'collectable',\n",
       " 'picture',\n",
       " 'the',\n",
       " 'history',\n",
       " 'of',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'archive',\n",
       " 'on',\n",
       " 'which',\n",
       " 'are',\n",
       " 'still',\n",
       " 'collected',\n",
       " 'by',\n",
       " 'cliff',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 's',\n",
       " 'colour',\n",
       " 'stella',\n",
       " 'and',\n",
       " 'rose',\n",
       " 's',\n",
       " 'in',\n",
       " 'they',\n",
       " 'p',\n",
       " 'g',\n",
       " 's',\n",
       " 'first',\n",
       " 'book',\n",
       " 'the',\n",
       " 'and',\n",
       " 'went',\n",
       " 'on',\n",
       " 'to',\n",
       " 'produce',\n",
       " 'many',\n",
       " 'of',\n",
       " 'his',\n",
       " 'early',\n",
       " 'works',\n",
       " 'in',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'both',\n",
       " 'helm',\n",
       " 'and',\n",
       " 'later',\n",
       " 'the',\n",
       " 'pica',\n",
       " 'press',\n",
       " 'of',\n",
       " 'the',\n",
       " 'helm',\n",
       " 'identification',\n",
       " 'from',\n",
       " 'helm',\n",
       " 'fob',\n",
       " 'out',\n",
       " 'of',\n",
       " 'business',\n",
       " 'in',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'was',\n",
       " 'by',\n",
       " 'which',\n",
       " 'continued',\n",
       " 'the',\n",
       " 'former',\n",
       " 's',\n",
       " 'range',\n",
       " 'of',\n",
       " 'reference',\n",
       " 'works',\n",
       " 'in',\n",
       " 'june',\n",
       " 't',\n",
       " 'a',\n",
       " 'd',\n",
       " 'and',\n",
       " 'their',\n",
       " 'back',\n",
       " 'list',\n",
       " 'of',\n",
       " 'around',\n",
       " 'ornithology',\n",
       " 'were',\n",
       " 'acquired',\n",
       " 'from',\n",
       " 'science',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'drama',\n",
       " 'from',\n",
       " 'in',\n",
       " 'and',\n",
       " 'acquired',\n",
       " 'from',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'in',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'music',\n",
       " 'list',\n",
       " 'to',\n",
       " 'collins',\n",
       " 'learning',\n",
       " 'a',\n",
       " 'division',\n",
       " 'of',\n",
       " 'notable',\n",
       " 'thumb',\n",
       " 'world',\n",
       " 'from',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 's',\n",
       " 'general',\n",
       " 'atlas',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'black',\n",
       " 's',\n",
       " 'medical',\n",
       " 'dictionary',\n",
       " 's',\n",
       " 'who',\n",
       " 's',\n",
       " 'who',\n",
       " 'yearbook',\n",
       " 'book',\n",
       " 'series',\n",
       " 'artist',\n",
       " 's',\n",
       " 'sketch',\n",
       " 'book',\n",
       " 'series',\n",
       " 'ballet',\n",
       " 'pocket',\n",
       " 'series',\n",
       " 'then',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'ballet',\n",
       " 'pocket',\n",
       " 'series',\n",
       " 'then',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'book',\n",
       " 'series',\n",
       " 'list',\n",
       " 'black',\n",
       " 's',\n",
       " 'black',\n",
       " 's',\n",
       " 'junior',\n",
       " 'reference',\n",
       " 'black',\n",
       " 's',\n",
       " 'novel',\n",
       " 'black',\n",
       " 's',\n",
       " 'novel',\n",
       " 'library',\n",
       " 'black',\n",
       " 's',\n",
       " 'popular',\n",
       " 'series',\n",
       " 'of',\n",
       " 'colour',\n",
       " 'black',\n",
       " 's',\n",
       " 'school',\n",
       " 'history',\n",
       " 'black',\n",
       " 's',\n",
       " 'water',\n",
       " 'colour',\n",
       " 'series',\n",
       " 'colour',\n",
       " 'the',\n",
       " 'net',\n",
       " 'series',\n",
       " 'colour',\n",
       " 'the',\n",
       " 'net',\n",
       " 'series',\n",
       " 'colour',\n",
       " 'the',\n",
       " 'net',\n",
       " 'series',\n",
       " 'colour',\n",
       " 'the',\n",
       " 'net',\n",
       " 'series',\n",
       " 'of',\n",
       " 'to',\n",
       " 'day',\n",
       " 'ecclesiastical',\n",
       " 'history',\n",
       " 'of',\n",
       " 'general',\n",
       " 'editor',\n",
       " 'j',\n",
       " 'c',\n",
       " 'the',\n",
       " 'fascination',\n",
       " 'of',\n",
       " 'walter',\n",
       " 'and',\n",
       " 'g',\n",
       " 'e',\n",
       " 'the',\n",
       " 'fascination',\n",
       " 'of',\n",
       " 'and',\n",
       " 'black',\n",
       " 'the',\n",
       " 'fascination',\n",
       " 'of',\n",
       " 'series',\n",
       " 'archive',\n",
       " 'guild',\n",
       " 'text',\n",
       " 'how',\n",
       " 'and',\n",
       " 'why',\n",
       " 'series',\n",
       " 'know',\n",
       " 'the',\n",
       " 'game',\n",
       " 'the',\n",
       " 'making',\n",
       " 'of',\n",
       " 'the',\n",
       " 'making',\n",
       " 'of',\n",
       " 'the',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'book',\n",
       " 'series',\n",
       " 'list',\n",
       " 'at',\n",
       " 'ancient',\n",
       " 'at',\n",
       " 'great',\n",
       " 'at',\n",
       " 'great',\n",
       " 'at',\n",
       " 'great',\n",
       " 'men',\n",
       " 'at',\n",
       " 'great',\n",
       " 'at',\n",
       " 'history',\n",
       " 'at',\n",
       " 'at',\n",
       " 'many',\n",
       " 'at',\n",
       " 'nature',\n",
       " 'at',\n",
       " 'nature',\n",
       " 'for',\n",
       " 'little',\n",
       " 'for',\n",
       " 'little',\n",
       " 'people',\n",
       " 'the',\n",
       " 'series',\n",
       " 'sometimes',\n",
       " 'miscellaneous',\n",
       " 'series',\n",
       " 'social',\n",
       " 'life',\n",
       " 'in',\n",
       " 'nautical',\n",
       " 'drama',\n",
       " 't',\n",
       " 'a',\n",
       " 'd',\n",
       " 'almanac',\n",
       " 'further',\n",
       " 'reading',\n",
       " 'colin',\n",
       " 'the',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'colour',\n",
       " 'a',\n",
       " 'collector',\n",
       " 's',\n",
       " 'guide',\n",
       " 'and',\n",
       " 'bibliography',\n",
       " 'shaw',\n",
       " 'external',\n",
       " 'links',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'archive',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'site',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'on',\n",
       " 'history',\n",
       " 'of',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'archive',\n",
       " 'copy',\n",
       " 'corporate',\n",
       " 'structure',\n",
       " 'corporate',\n",
       " 'history',\n",
       " 's',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'colour',\n",
       " 'twenty',\n",
       " 'shilling',\n",
       " 'series',\n",
       " 'of',\n",
       " 'a',\n",
       " 'c',\n",
       " 'black',\n",
       " 'university',\n",
       " 'of',\n",
       " 'reading',\n",
       " 'the',\n",
       " 'black',\n",
       " 'category',\n",
       " 'category',\n",
       " 'ornithological',\n",
       " 'category',\n",
       " 'established',\n",
       " 'in',\n",
       " 'category',\n",
       " 'in',\n",
       " 'category',\n",
       " 'in',\n",
       " 'category',\n",
       " 'based',\n",
       " 'in',\n",
       " 'category',\n",
       " 'history',\n",
       " 'of',\n",
       " 'category',\n",
       " 'based',\n",
       " 'in',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'category',\n",
       " 'book',\n",
       " 'of',\n",
       " 'category',\n",
       " 'and',\n",
       " 'a',\n",
       " 'f',\n",
       " 'first',\n",
       " 'spinning',\n",
       " 'cotton',\n",
       " 'mill',\n",
       " 'established',\n",
       " 'by',\n",
       " 'and',\n",
       " 'frank',\n",
       " 'in',\n",
       " 'the',\n",
       " 'year',\n",
       " 'early',\n",
       " 'history',\n",
       " 'a',\n",
       " 'f',\n",
       " 'were',\n",
       " 'born',\n",
       " 'in',\n",
       " 'the',\n",
       " 'year',\n",
       " 'and',\n",
       " 'respectively',\n",
       " 'in',\n",
       " 'a',\n",
       " 'farmer',\n",
       " 'family',\n",
       " 'in',\n",
       " 'they',\n",
       " 'traveled',\n",
       " 'to',\n",
       " 'during',\n",
       " 'century',\n",
       " 'and',\n",
       " 'landed',\n",
       " 'in',\n",
       " 'they',\n",
       " 'the',\n",
       " 'business',\n",
       " 'of',\n",
       " 'cotton',\n",
       " 'and',\n",
       " 'established',\n",
       " 'the',\n",
       " 'first',\n",
       " 'cotton',\n",
       " 'press',\n",
       " 'mill',\n",
       " 'in',\n",
       " 'near',\n",
       " 'they',\n",
       " 'export',\n",
       " 'business',\n",
       " 'in',\n",
       " 'cotton',\n",
       " 'in',\n",
       " 's',\n",
       " 'a',\n",
       " 'hydro',\n",
       " 'electric',\n",
       " 'project',\n",
       " 'in',\n",
       " 'frank',\n",
       " 'in',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'year',\n",
       " 'and',\n",
       " 'their',\n",
       " 'memorial',\n",
       " 'was',\n",
       " 'by',\n",
       " 'sir',\n",
       " 'the',\n",
       " 'then',\n",
       " 'director',\n",
       " 'at',\n",
       " 'in',\n",
       " 'list',\n",
       " 'of',\n",
       " 'a',\n",
       " 'f',\n",
       " 'cotton',\n",
       " 'press',\n",
       " 'in',\n",
       " 'now',\n",
       " 'as',\n",
       " 'coral',\n",
       " 'in',\n",
       " 'in',\n",
       " 'category',\n",
       " 'cotton',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'limited',\n",
       " 'is',\n",
       " 'an',\n",
       " 'engineering',\n",
       " 'firm',\n",
       " 'and',\n",
       " 'locomotive',\n",
       " 'manufacturer',\n",
       " 'in',\n",
       " 'new',\n",
       " 'in',\n",
       " 'history',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'was',\n",
       " 'established',\n",
       " 'in',\n",
       " 'in',\n",
       " 'street',\n",
       " 'by',\n",
       " 'price',\n",
       " 'and',\n",
       " 'price',\n",
       " 'two',\n",
       " 'from',\n",
       " 'stroud',\n",
       " 'they',\n",
       " 'built',\n",
       " 'almost',\n",
       " 'flax',\n",
       " 'milling',\n",
       " 'in',\n",
       " 'their',\n",
       " 'first',\n",
       " 'year',\n",
       " 'book',\n",
       " 'june',\n",
       " 'page',\n",
       " 'the',\n",
       " 'also',\n",
       " 'built',\n",
       " 'machinery',\n",
       " 'for',\n",
       " 'gold',\n",
       " 'they',\n",
       " 'to',\n",
       " 'the',\n",
       " 'gold',\n",
       " 'in',\n",
       " 'setting',\n",
       " 'up',\n",
       " 'in',\n",
       " 'beach',\n",
       " 'road',\n",
       " 'and',\n",
       " 'the',\n",
       " 'works',\n",
       " 'in',\n",
       " 'after',\n",
       " 'building',\n",
       " 'and',\n",
       " 'trucks',\n",
       " 'there',\n",
       " 'for',\n",
       " 'the',\n",
       " 'public',\n",
       " 'works',\n",
       " 'department',\n",
       " 'the',\n",
       " 'firm',\n",
       " 's',\n",
       " 'ownership',\n",
       " 'was',\n",
       " 'transferred',\n",
       " 'to',\n",
       " 'a',\n",
       " 'limited',\n",
       " 'liability',\n",
       " 'company',\n",
       " 'in',\n",
       " 'ownership',\n",
       " 'thumb',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'early',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'limited',\n",
       " 'under',\n",
       " 'family',\n",
       " 'management',\n",
       " 'until',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'bought',\n",
       " 'by',\n",
       " 'wellington',\n",
       " 'cable',\n",
       " 'company',\n",
       " 'the',\n",
       " 'two',\n",
       " 'then',\n",
       " 'board',\n",
       " 'but',\n",
       " 'kept',\n",
       " 'their',\n",
       " 'separate',\n",
       " 'cable',\n",
       " 'bought',\n",
       " 'downer',\n",
       " 'in',\n",
       " 'and',\n",
       " 'in',\n",
       " 'cable',\n",
       " 'was',\n",
       " 'cable',\n",
       " 'price',\n",
       " 'downer',\n",
       " 'limited',\n",
       " 'in',\n",
       " 'the',\n",
       " 'staff',\n",
       " 'of',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'alone',\n",
       " 'was',\n",
       " 'in',\n",
       " 'excess',\n",
       " 'of',\n",
       " 'people',\n",
       " 'its',\n",
       " 'head',\n",
       " 'office',\n",
       " 'was',\n",
       " 'in',\n",
       " 'street',\n",
       " 'beach',\n",
       " 'road',\n",
       " 'was',\n",
       " 'as',\n",
       " 'a',\n",
       " 'branch',\n",
       " 'the',\n",
       " 'new',\n",
       " 'business',\n",
       " 'who',\n",
       " 's',\n",
       " 'who',\n",
       " 'edition',\n",
       " 'wellington',\n",
       " 'undated',\n",
       " 'in',\n",
       " 'corporate',\n",
       " 'raider',\n",
       " 'control',\n",
       " 'of',\n",
       " 'the',\n",
       " 'group',\n",
       " 'parent',\n",
       " 'cable',\n",
       " 'price',\n",
       " 'downer',\n",
       " 'and',\n",
       " 'broke',\n",
       " 'the',\n",
       " 'group',\n",
       " 'back',\n",
       " 'into',\n",
       " 'its',\n",
       " 'three',\n",
       " 'separate',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'beach',\n",
       " 'road',\n",
       " 'was',\n",
       " 'until',\n",
       " 'liquidation',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'group',\n",
       " 'based',\n",
       " 'in',\n",
       " 'mount',\n",
       " 'wellington',\n",
       " 'and',\n",
       " 'from',\n",
       " 'nelson',\n",
       " 'by',\n",
       " 'history',\n",
       " 'us',\n",
       " 'group',\n",
       " 'in',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'was',\n",
       " 'in',\n",
       " 'administration',\n",
       " 'with',\n",
       " 'the',\n",
       " 'loss',\n",
       " 'of',\n",
       " 'new',\n",
       " 'rail',\n",
       " 'engineering',\n",
       " 'firm',\n",
       " 'after',\n",
       " 'nearly',\n",
       " 'the',\n",
       " 'railway',\n",
       " 'magazine',\n",
       " 'issue',\n",
       " 'page',\n",
       " 'the',\n",
       " 'business',\n",
       " 'was',\n",
       " 'bought',\n",
       " 'from',\n",
       " 'the',\n",
       " 'administrator',\n",
       " 'by',\n",
       " 'reeve',\n",
       " 'in',\n",
       " 'reeve',\n",
       " 'had',\n",
       " 'been',\n",
       " 'unable',\n",
       " 'to',\n",
       " 'sell',\n",
       " 'the',\n",
       " 'land',\n",
       " 'and',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'foundry',\n",
       " 'sold',\n",
       " 'and',\n",
       " 'to',\n",
       " 'be',\n",
       " 'stuff',\n",
       " 'march',\n",
       " 'the',\n",
       " 'business',\n",
       " 'now',\n",
       " 'with',\n",
       " 'a',\n",
       " 'reduced',\n",
       " 'under',\n",
       " 'reeve',\n",
       " 's',\n",
       " 'ownership',\n",
       " 'a',\n",
       " 'g',\n",
       " 'price',\n",
       " 'produced',\n",
       " 'water',\n",
       " 'turbines',\n",
       " 'under',\n",
       " 'the',\n",
       " 'patent',\n",
       " 'allan',\n",
       " 'and',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'efficient',\n",
       " 'turbine',\n",
       " 'it',\n",
       " 'in',\n",
       " 'initially',\n",
       " 'and',\n",
       " 'sold',\n",
       " 'the',\n",
       " 'turbines',\n",
       " 'to',\n",
       " 'gold',\n",
       " 'mine',\n",
       " 'in',\n",
       " 'the',\n",
       " 'and',\n",
       " 'later',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61c6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421316"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fcf113",
   "metadata": {},
   "source": [
    "#### In this step, we generate a vocabulary by collecting unique tokens from the tokenized text data. Each word in the vocabulary is then assigned an index to facilitate numerical representation. These indices are used to map between words and their corresponding numerical identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c866f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(set(token))\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "vocab_size = len(vocabulary)\n",
    "output_size = vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959763f",
   "metadata": {},
   "source": [
    "#### Train a Word2Vec model on the tokenized text data (token). The trained model is then saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ec4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=[token], vector_size=100, window=2, min_count=1, workers=multiprocessing.cpu_count())\n",
    "\n",
    "# Save the trained Word2Vec model for later use\n",
    "word2vec_model.save(\"word2vec_model.bin\")\n",
    "\n",
    "embedding_dim = word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced3740",
   "metadata": {},
   "source": [
    "#### Generate training sequences by sliding a window of length sequence_length (10) over the tokenized text (tokens). Each sequence consists of sequence_length tokens as input and the next token as the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e04f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(tokens, sequence_length=10):\n",
    "    sequences = []\n",
    "    for i in range(len(tokens) - sequence_length):\n",
    "        seq = tokens[i:i + sequence_length + 1]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "train_seq = create_training_sequences(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5778dc",
   "metadata": {},
   "source": [
    "#### The second function, sequence_to_embeddings, converts the generated sequences into embedded representations using the pre-trained Word2Vec model (word2vec_model). It retrieves word embeddings for each token in the sequences, filtering out tokens not present in the Word2Vec model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e077dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_embeddings(sequences, word2vec_model):\n",
    "    embedded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        embedded_seq = [word2vec_model.wv[word] for word in sequence if word in word2vec_model.wv]\n",
    "        embedded_sequences.append(embedded_seq)\n",
    "    return embedded_sequences\n",
    "\n",
    "embedded_seq = sequence_to_embeddings(train_seq, word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a420f72",
   "metadata": {},
   "source": [
    "#### Define a LSTM model for sequence prediction tasks. The model consists of multiple layers: an LSTM layer with num_layers stacked LSTM cells, followed by a sequence of fully connected (dense) layers (fc_layers) with ReLU activation functions. The final output layer produces predictions for the next token in the sequence. The dropout layer is applied to prevent overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06595965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, num_fc_layers=2, fc_hidden_size=64):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(hidden_size, fc_hidden_size)])\n",
    "        for _ in range(num_fc_layers - 1):\n",
    "            self.fc_layers.append(nn.Linear(fc_hidden_size, fc_hidden_size))\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Only take the output from the final timestep\n",
    "        x = self.dropout(lstm_out[:, -1, :])\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.relu(fc_layer(x))\n",
    "        \n",
    "        # Final output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cab533",
   "metadata": {},
   "source": [
    "#### Calculate the accuracy of the model predictions given the model outputs and the corresponding ground truth labels. Compare the predicted labels with the actual labels to count the number of correct predictions. Finally, compute the accuracy by dividing the number of correct predictions by the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61c0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72036ab",
   "metadata": {},
   "source": [
    "#### Prepare the Predictor and Target for training the model. We convert the embedded sequences (embedded_seq) into PyTorch tensors (X) of data and their corresponding labels (y). The labels represent the index of the next word in each sequence. We split the data into training and testing sets using the train_test_split function, allocating 20% of the data for testing and the remaining 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4624fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30816\\2032664818.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  X = torch.tensor(embedded_seq, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64  # LSTM units\n",
    "\n",
    "X = torch.tensor(embedded_seq, dtype=torch.float32)\n",
    "y = torch.tensor([word_to_index.get(seq[-1], 0) for seq in train_seq], dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f71d3",
   "metadata": {},
   "source": [
    "#### Initialize the LSTM model (model) with the specified input size, hidden size, and output size. We also define the optimizer (optimizer) using the Adam optimizer with a learning rate of 0.001. Additionally, we specify the loss function (criterion) as the CrossEntropyLoss, which is commonly used for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7152f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_size=embedding_dim, hidden_size=hidden_size, output_size=output_size)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fb0a0",
   "metadata": {},
   "source": [
    "#### Train the LSTM model using the specified train_loader for a given number of epochs (num_epochs). It iterates through the data loader, calculates the loss and accuracy for each batch, performs backpropagation, and updates the model parameters using the optimizer. It also implements early stopping based on the validation loss, with a patience parameter to control the number of epochs to wait for improvement. Additionally, it utilizes a learning rate scheduler to adjust the learning rate during training based on the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85752326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=12):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    patience = 5\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        total_samples = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # Calculate accuracy\n",
    "            accuracy = calculate_accuracy(outputs, labels)\n",
    "            running_accuracy += accuracy * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = running_accuracy / total_samples\n",
    "        print(f'Epoch {epoch+1} Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_accuracy = epoch_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve == patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "            \n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "    print(f'Best Loss: {best_loss:.4f}, Best Accuracy: {best_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24d066",
   "metadata": {},
   "source": [
    "#### Prepare the training dataset by creating a TensorDataset from the training features (X_train) and labels (y_train). Then, create a DataLoader object called train_loader to iterate over the training data in batches during model training. The batch_size parameter specifies the number of samples in each batch, and shuffle=True indicates that the data will be shuffled before creating batches to improve training efficiency and model generalization. Train the model after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1427e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f24f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.5109, Accuracy: 0.2851\n",
      "Epoch 2 Loss: 3.7365, Accuracy: 0.3720\n",
      "Epoch 3 Loss: 3.5140, Accuracy: 0.3973\n",
      "Epoch 4 Loss: 3.3521, Accuracy: 0.4134\n",
      "Epoch 5 Loss: 3.2351, Accuracy: 0.4232\n",
      "Epoch 6 Loss: 3.1403, Accuracy: 0.4321\n",
      "Epoch 7 Loss: 3.0676, Accuracy: 0.4387\n",
      "Epoch 8 Loss: 3.0080, Accuracy: 0.4447\n",
      "Epoch 9 Loss: 2.9552, Accuracy: 0.4491\n",
      "Epoch 10 Loss: 2.9000, Accuracy: 0.4580\n",
      "Epoch 11 Loss: 2.8530, Accuracy: 0.4640\n",
      "Epoch 12 Loss: 2.8134, Accuracy: 0.4713\n",
      "Best Loss: 2.8134, Best Accuracy: 0.4713\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the learning rate scheduler\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98e784",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the trained LSTM model on the test dataset. It iterates over batches of test data using the provided test_loader and computes the average loss and accuracy for the entire test dataset. The model is set to evaluation mode (model.eval()) to disable dropout and batch normalization layers during evaluation. Finally, it prints the average test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b402ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.5068, Test Accuracy: 0.4586\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = calculate_accuracy(outputs, labels)\n",
    "            test_accuracy += accuracy * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    avg_test_accuracy = test_accuracy / total_samples\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_accuracy:.4f}')\n",
    "\n",
    "# Create test dataset and data loader\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f644315",
   "metadata": {},
   "source": [
    "#### Generate the top 6 predicted next words given a seed text using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ddcb48d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predicted next words: ['comedy', 'how', 'war', 'smith', 'light', 'wife']\n"
     ]
    }
   ],
   "source": [
    "def predict_next_words(seed_text, model, word_to_index, index_to_word, word2vec_model, sequence_length, top_n=6):\n",
    "    seed_tokens = seed_text.split()\n",
    "    embedded_tokens = [word2vec_model.wv[word] for word in seed_tokens if word in word2vec_model.wv]\n",
    "    # Pad the sequence if needed\n",
    "    if len(embedded_tokens) < sequence_length:\n",
    "        padding = [np.zeros(word2vec_model.vector_size)] * (sequence_length - len(embedded_tokens))\n",
    "        embedded_tokens = padding + embedded_tokens\n",
    "    token_tensor = torch.tensor([embedded_tokens], dtype=torch.float32)\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    token_tensor = token_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(token_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        top_probabilities, top_indices = torch.topk(probabilities, top_n, dim=1)\n",
    "\n",
    "    top_words = [index_to_word[index.item()] for index in top_indices[0]]\n",
    "    top_probabilities = top_probabilities.squeeze().tolist()\n",
    "\n",
    "    return list((top_words))\n",
    "\n",
    "# Example usage\n",
    "sample_input = \"I am going to University of Arizona to study\"\n",
    "top_predicted_words = predict_next_words(sample_input, model, word_to_index, index_to_word, word2vec_model, sequence_length=10, top_n=6)\n",
    "\n",
    "print(\"Top predicted next words:\", top_predicted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7437256",
   "metadata": {},
   "source": [
    "#### Define a Vanilla RNN model architecture. Initialize the model with specified input_size, hidden_size, and output_size. Then, define the loss function as Cross Entropy Loss and the optimizer as Adam. Additionally, a learning rate scheduler is defined to adjust the learning rate during training. Finally, reduce the learning rate if the validation loss does not improve for the specified patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cd057dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.625983238220215, Training Accuracy: 0.25695754856932623\n",
      "Epoch 2, Loss: 2.9773545265197754, Training Accuracy: 0.5054918645636771\n",
      "Epoch 3, Loss: 0.5300394296646118, Training Accuracy: 0.7116756269211142\n",
      "Epoch 4, Loss: 0.2931666374206543, Training Accuracy: 0.842076998848815\n",
      "Epoch 5, Loss: 0.14371295273303986, Training Accuracy: 0.9033657326639846\n",
      "Epoch 6, Loss: 0.401125431060791, Training Accuracy: 0.9350707919440785\n",
      "Epoch 7, Loss: 1.1446062326431274, Training Accuracy: 0.9531693191393409\n",
      "Epoch 8, Loss: 0.7587936520576477, Training Accuracy: 0.9633193292270446\n",
      "Epoch 9, Loss: 0.1993234008550644, Training Accuracy: 0.9722439800144789\n",
      "Epoch 10, Loss: 0.10240236669778824, Training Accuracy: 0.9728403413204211\n",
      "Epoch 11, Loss: 0.20833894610404968, Training Accuracy: 0.9730836329974721\n",
      "Epoch 12, Loss: 0.07712671905755997, Training Accuracy: 0.9732824200994529\n"
     ]
    }
   ],
   "source": [
    "# Define Vanilla RNN model\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out \n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "hidden_size = 64\n",
    "output_size = vocab_size\n",
    "\n",
    "# Initialize the model\n",
    "model_rnn = VanillaRNN(input_size=embedding_dim, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_rnn.parameters(), lr=0.001)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Define DataLoader for training set\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(12):  # Train for 10 epochs as an example\n",
    "    model_rnn.train()  # Set the model to training mode\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_rnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    # Print training accuracy and loss for every epoch\n",
    "    train_accuracy = total_correct / total_samples\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Training Accuracy: {train_accuracy}')\n",
    "    \n",
    "    # Reduce learning rate if validation loss does not improve for patience number of epochs\n",
    "    scheduler.step(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c48407",
   "metadata": {},
   "source": [
    "#### Evaluate the Vanilla RNN model on the test set. Initialize a DataLoader for the test set with a batch size of 32. Set the model to evaluation mode using model_rnn.eval(). Within a no_grad context, it iterates over the test loader, calculates the loss and accuracy for each batch, and aggregates them over the entire test set. Print the average test loss and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f157f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5311, Test Accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoader for test set\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model_rnn.eval()\n",
    "test_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_rnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)  # Multiply by batch size\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = total_correct / total_samples\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b14b31",
   "metadata": {},
   "source": [
    "#### Generate the top 6 predicted next words given a seed text using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "258f7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, model, word2vec_model, word_to_index, index_to_word, top_k=6):\n",
    "    # Tokenize input text\n",
    "    input_tokens = word_tokenize(input_text.lower())\n",
    "    \n",
    "    # Convert tokens to embeddings\n",
    "    input_embeddings = [word2vec_model.wv[word] for word in input_tokens if word in word2vec_model.wv]\n",
    "    \n",
    "    # Convert embeddings to PyTorch tensor\n",
    "    input_tensor = torch.tensor([input_embeddings], dtype=torch.float32)\n",
    "    \n",
    "    # Get predictions from the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "    \n",
    "    # Get top k predictions\n",
    "    _, top_indices = torch.topk(outputs, top_k)\n",
    "    \n",
    "    # Convert indices to words\n",
    "    predicted_words = [index_to_word[idx.item()] for idx in top_indices[0]]\n",
    "    \n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3479e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted words: ['study', 'guardian', 'overcome', 'marry', 'behave', 'invite']\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I am going to University of Arizona to study\"\n",
    "predicted_words = predict_next_word(input_text, model_rnn, word2vec_model, word_to_index, index_to_word, top_k=6)\n",
    "print(\"Predicted words:\", predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6abc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
